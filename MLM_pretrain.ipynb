{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be08452f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trishad2/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2070729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b068fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/trishad2/PyTrial\n"
     ]
    }
   ],
   "source": [
    "%cd /home/trishad2/PyTrial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb3e300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    dev = \"cuda:0\" \n",
    "else:\n",
    "    dev = \"cpu\" \n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e3c0c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'lung/data/train_test_valid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d054a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path1 = 'lung/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6a4409",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'lung/model/train_test_valid/with_biobert/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24b31ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 13, 83]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= pd.read_csv(data_path1+'full_datav3.csv',  index_col = 0)\n",
    "\n",
    "ae_cols = [i for i in data.columns if i.startswith('AE_')]\n",
    "med_cols = [i for i in data.columns if i.startswith('CM_')]\n",
    "treatment_cols = [i for i in data.columns if i.startswith('Treatment_')]\n",
    "feature_cols =    treatment_cols + med_cols + ae_cols\n",
    "vocab_size = [len(treatment_cols), len(med_cols), len(ae_cols)]\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6746b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_id_dict={}\n",
    "for i in range(len(treatment_cols)):\n",
    "    treatment_id_dict[i]=treatment_cols[i].split('_')[1]\n",
    "    \n",
    "med_id_dict={}\n",
    "for i in range(len(med_cols)):\n",
    "    med_id_dict[i]=med_cols[i].split('_')[1]\n",
    "    \n",
    "ae_id_dict={}\n",
    "for i in range(len(ae_cols)):\n",
    "    ae_id_dict[i]=ae_cols[i].split('_')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97d42687",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_emb_dict = pd.read_pickle(data_path1+'ae_emb_dict.pickle')\n",
    "\n",
    "med_emb_dict = pd.read_pickle(data_path1+'med_emb_dict.pickle')\n",
    "\n",
    "treatment_emb_dict = pd.read_pickle(data_path1+'treatment_emb_dict.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47917af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = []\n",
    "treatment_cols =  [col for col in data if col.startswith('Treatment_')]\n",
    "medication_cols = [col for col in data if col.startswith('CM_')]\n",
    "ae_cols = [col for col in data if col.startswith('AE_')]\n",
    "for i in data.People.unique():\n",
    "  sample=[]\n",
    "  temp = data[data['People']==i]\n",
    "  for index, row in temp.iterrows():\n",
    "    visit=[]\n",
    "    visit.append(np.nonzero(row[treatment_cols].to_list())[0].tolist())\n",
    "    visit.append(np.nonzero(row[medication_cols].to_list())[0].tolist())\n",
    "    visit.append(np.nonzero(row[ae_cols].to_list())[0].tolist())\n",
    "    sample.append(visit)\n",
    "  visits.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "874323ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_biobert=[]\n",
    "#i for a patient\n",
    "for i in range(len(visits)):\n",
    "    \n",
    "    visits_per_patient=[]\n",
    "    #j for a visit of that patient\n",
    "    for j in range(len(visits[i])):\n",
    "        visit=[]\n",
    "        #k for an event type of that visit\n",
    "        for k in range(len(visits[i][j])):\n",
    "            \n",
    "            #l for an event of that event type\n",
    "            for l in visits[i][j][k]:\n",
    "                if k==0:\n",
    "                    #print(treatment_id_dict[l])\n",
    "                    visit.append(treatment_emb_dict[treatment_id_dict[l]])\n",
    "                if k==1:\n",
    "                    #print(med_code_dict[med_id_dict[l]])\n",
    "                    visit.append(med_emb_dict[l])\n",
    "                if k ==2:\n",
    "                    #print(ae_code_dict[ae_id_dict[l]])\n",
    "                    visit.append(ae_emb_dict[l])\n",
    "        visit = torch.cat(visit)\n",
    "        visit = visit.mean(dim = 0).detach().numpy()\n",
    "        #print(visit.shape)\n",
    "        visits_per_patient.append(visit)\n",
    "    visits_biobert.append(visits_per_patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a877993c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(527, 9, 768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(visits_biobert), len(visits_biobert[1]), len(visits_biobert[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ca55874",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "950fce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs):\n",
    "        # your code here\n",
    "        self.x = seqs\n",
    "        #self.y = labels\n",
    "        #self.baseline = baseline\n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.x[index]#, self.y[index], self.baseline[index]\n",
    "        \n",
    "\n",
    "train_dataset = CustomDataset(visits_biobert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9d8a6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_med(data):\n",
    "\n",
    "    sequences = data\n",
    "\n",
    "    #y = torch.tensor(labels, dtype=torch.float)\n",
    "    #baseline = torch.tensor(np.vstack(baselines), dtype=torch.float)\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    max_num_visits = max(num_visits)\n",
    "    lengths = [len(x) for x in sequences]\n",
    "\n",
    "    x = torch.zeros((num_patients, max_num_visits, vocab_size), dtype=torch.float)    \n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "                x[i_patient][j_visit] = torch.from_numpy(np.array(visit))\n",
    "                \n",
    "    #print(torch.sum(x, dim=-1)!=0)\n",
    "    masks = torch.sum(x, dim=-1) != 0\n",
    "    \n",
    "    return x, masks, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6883f282",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, masks, lengths = collate_fn_med(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc0bd684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8340560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(MLMEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        self.pos_encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(hidden_size, nhead=1, dim_feedforward = hidden_size,),\n",
    "            num_layers\n",
    "        )\n",
    "\n",
    "        self.mlm_head = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "\n",
    "        x = x.permute(1, 0, 2)\n",
    "        output = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        \n",
    "        logits = self.mlm_head(output)\n",
    "        return output, logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a17de99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_encoder = MLMEncoder(input_size = vocab_size, hidden_size = 256, num_layers = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e676be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretraining hyperparameters\n",
    "pretrain_epochs = 100\n",
    "pretrain_optimizer = torch.optim.Adam(pretrained_encoder.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73f7a021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining Epoch 1 MLM Loss: 0.46994689106941223\n",
      "Pretraining Epoch 2 MLM Loss: 0.3977401852607727\n",
      "Pretraining Epoch 3 MLM Loss: 0.3380074203014374\n",
      "Pretraining Epoch 4 MLM Loss: 0.3105923533439636\n",
      "Pretraining Epoch 5 MLM Loss: 0.27812159061431885\n",
      "Pretraining Epoch 6 MLM Loss: 0.25639307498931885\n",
      "Pretraining Epoch 7 MLM Loss: 0.23466560244560242\n",
      "Pretraining Epoch 8 MLM Loss: 0.21940608322620392\n",
      "Pretraining Epoch 9 MLM Loss: 0.19098100066184998\n",
      "Pretraining Epoch 10 MLM Loss: 0.17713876068592072\n",
      "Pretraining Epoch 11 MLM Loss: 0.18379372358322144\n",
      "Pretraining Epoch 12 MLM Loss: 0.15356020629405975\n",
      "Pretraining Epoch 13 MLM Loss: 0.14176160097122192\n",
      "Pretraining Epoch 14 MLM Loss: 0.14829622209072113\n",
      "Pretraining Epoch 15 MLM Loss: 0.12318902462720871\n",
      "Pretraining Epoch 16 MLM Loss: 0.1150459423661232\n",
      "Pretraining Epoch 17 MLM Loss: 0.11476486176252365\n",
      "Pretraining Epoch 18 MLM Loss: 0.09963613003492355\n",
      "Pretraining Epoch 19 MLM Loss: 0.09363079816102982\n",
      "Pretraining Epoch 20 MLM Loss: 0.11118687689304352\n",
      "Pretraining Epoch 21 MLM Loss: 0.08389159291982651\n",
      "Pretraining Epoch 22 MLM Loss: 0.078253373503685\n",
      "Pretraining Epoch 23 MLM Loss: 0.07150153815746307\n",
      "Pretraining Epoch 24 MLM Loss: 0.06728589534759521\n",
      "Pretraining Epoch 25 MLM Loss: 0.07944737374782562\n",
      "Pretraining Epoch 26 MLM Loss: 0.07995129376649857\n",
      "Pretraining Epoch 27 MLM Loss: 0.055648818612098694\n",
      "Pretraining Epoch 28 MLM Loss: 0.06306906789541245\n",
      "Pretraining Epoch 29 MLM Loss: 0.04987179487943649\n",
      "Pretraining Epoch 30 MLM Loss: 0.04557659849524498\n",
      "Pretraining Epoch 31 MLM Loss: 0.04419020190834999\n",
      "Pretraining Epoch 32 MLM Loss: 0.04035455733537674\n",
      "Pretraining Epoch 33 MLM Loss: 0.042985159903764725\n",
      "Pretraining Epoch 34 MLM Loss: 0.03713051602244377\n",
      "Pretraining Epoch 35 MLM Loss: 0.04762079939246178\n",
      "Pretraining Epoch 36 MLM Loss: 0.03235017880797386\n",
      "Pretraining Epoch 37 MLM Loss: 0.03079606033861637\n",
      "Pretraining Epoch 38 MLM Loss: 0.03503752127289772\n",
      "Pretraining Epoch 39 MLM Loss: 0.02809191308915615\n",
      "Pretraining Epoch 40 MLM Loss: 0.03566591441631317\n",
      "Pretraining Epoch 41 MLM Loss: 0.02577497810125351\n",
      "Pretraining Epoch 42 MLM Loss: 0.02831874042749405\n",
      "Pretraining Epoch 43 MLM Loss: 0.02779049426317215\n",
      "Pretraining Epoch 44 MLM Loss: 0.0341760627925396\n",
      "Pretraining Epoch 45 MLM Loss: 0.02246333658695221\n",
      "Pretraining Epoch 46 MLM Loss: 0.021508699283003807\n",
      "Pretraining Epoch 47 MLM Loss: 0.023053810000419617\n",
      "Pretraining Epoch 48 MLM Loss: 0.02036418579518795\n",
      "Pretraining Epoch 49 MLM Loss: 0.030095761641860008\n",
      "Pretraining Epoch 50 MLM Loss: 0.024775907397270203\n"
     ]
    }
   ],
   "source": [
    "batch_size = len(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_med)\n",
    "\n",
    "# Pretraining loop\n",
    "for epoch in range(pretrain_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for x, batch_mask, lengths in train_dataloader:  # Assuming you have a DataLoader for pretraining data\n",
    "        \n",
    "        pretrain_optimizer.zero_grad()\n",
    "        number_of_visits = x.shape[1]\n",
    "        \n",
    "\n",
    "        # Randomly select visits to mask out\n",
    "        mask_indices = random.sample(range(x.size(1)), k=1)  # Select 2 random visits to mask out\n",
    "\n",
    "        # Generate the mask tensor\n",
    "        mask = torch.ones(x.size(0), x.size(1)).bool()\n",
    "        mask[:, mask_indices] = False\n",
    "        \n",
    "        \n",
    "        # Replace visits with all zeroes based on the mask\n",
    "        masked_input_data = torch.where(mask.unsqueeze(-1), x, torch.full_like(x, float(-10.0)))#torch.zeros_like(x[0]))\n",
    "\n",
    "        and_mask = torch.logical_and(mask, batch_mask)\n",
    "\n",
    "        # Forward pass\n",
    "        output, logits = pretrained_encoder(masked_input_data, and_mask)\n",
    "\n",
    "        # Compute MLM loss\n",
    "        mlm_loss = F.mse_loss(logits, x.float(), reduction='none')\n",
    "        mlm_loss = mlm_loss.masked_select(batch_mask.unsqueeze(-1)).mean()\n",
    "\n",
    "        # Backward pass\n",
    "        mlm_loss.backward(retain_graph=True)\n",
    "        pretrain_optimizer.step()\n",
    "\n",
    "        total_loss += mlm_loss.item()\n",
    "\n",
    "    # Print average MLM loss for the epoch\n",
    "    print(f\"Pretraining Epoch {epoch+1} MLM Loss: {total_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a97a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pretrained encoder\n",
    "torch.save(pretrained_encoder.state_dict(), model_path+\"pretrained_encoder_MLM_biobert_e100.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3055ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082464f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe9cdb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c15e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
